{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "faa1831a-f81d-4656-a681-6e1eca4c46da",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\nCollecting dataclasses\n  Using cached dataclasses-0.6-py3-none-any.whl (14 kB)\nCollecting oauth2client\n  Using cached oauth2client-4.1.3-py2.py3-none-any.whl (98 kB)\nCollecting selectolax\n  Using cached selectolax-0.3.16-cp310-cp310-manylinux_2_17_x86_64.manylinux2014_x86_64.whl (5.5 MB)\nCollecting chompjs\n  Using cached chompjs-1.2.2-cp310-cp310-linux_x86_64.whl\nCollecting nested_lookup\n  Using cached nested_lookup-0.2.25-py3-none-any.whl\nRequirement already satisfied: tqdm in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (4.66.1)\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from oauth2client) (0.3.0)\nRequirement already satisfied: httplib2>=0.9.1 in /usr/lib/python3/dist-packages (from oauth2client) (0.20.2)\nRequirement already satisfied: six>=1.6.1 in /usr/lib/python3/dist-packages (from oauth2client) (1.16.0)\nRequirement already satisfied: pyasn1>=0.1.7 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from oauth2client) (0.5.0)\nRequirement already satisfied: rsa>=3.1.4 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from oauth2client) (4.9)\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /databricks/python3/lib/python3.10/site-packages (from httplib2>=0.9.1->oauth2client) (3.0.9)\nInstalling collected packages: selectolax, dataclasses, nested_lookup, chompjs, oauth2client\nSuccessfully installed chompjs-1.2.2 dataclasses-0.6 nested_lookup-0.2.25 oauth2client-4.1.3 selectolax-0.3.16\n\u001B[43mNote: you may need to restart the kernel using dbutils.library.restartPython() to use updated packages.\u001B[0m\n"
     ]
    }
   ],
   "source": [
    "%pip install dataclasses oauth2client selectolax chompjs nested_lookup tqdm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "edd7dfa4-dfb2-476e-8e3d-33e99b7f7bc0",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "stream",
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: oauth2client in /local_disk0/.ephemeral_nfs/envs/pythonEnv-f740537d-14ae-439d-8edd-f3e3f0e1d383/lib/python3.10/site-packages (4.1.3)\r\nRequirement already satisfied: gspread in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (4.0.1)\r\nCollecting curlify\r\n  Using cached curlify-2.2.1-py3-none-any.whl\r\nRequirement already satisfied: pyasn1>=0.1.7 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from oauth2client) (0.5.0)\r\nRequirement already satisfied: rsa>=3.1.4 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from oauth2client) (4.9)\r\nRequirement already satisfied: pyasn1-modules>=0.0.5 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from oauth2client) (0.3.0)\r\nRequirement already satisfied: httplib2>=0.9.1 in /usr/lib/python3/dist-packages (from oauth2client) (0.20.2)\r\nRequirement already satisfied: six>=1.6.1 in /usr/lib/python3/dist-packages (from oauth2client) (1.16.0)\r\nRequirement already satisfied: google-auth>=1.12.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from gspread) (2.23.0)\r\nRequirement already satisfied: google-auth-oauthlib>=0.4.1 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from gspread) (1.1.0)\r\nRequirement already satisfied: requests in /databricks/python3/lib/python3.10/site-packages (from curlify) (2.28.1)\r\nRequirement already satisfied: urllib3<2.0 in /databricks/python3/lib/python3.10/site-packages (from google-auth>=1.12.0->gspread) (1.26.11)\r\nRequirement already satisfied: cachetools<6.0,>=2.0.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from google-auth>=1.12.0->gspread) (5.3.1)\r\nRequirement already satisfied: requests-oauthlib>=0.7.0 in /local_disk0/.ephemeral_nfs/cluster_libraries/python/lib/python3.10/site-packages (from google-auth-oauthlib>=0.4.1->gspread) (1.3.1)\r\nRequirement already satisfied: pyparsing!=3.0.0,!=3.0.1,!=3.0.2,!=3.0.3,<4,>=2.4.2 in /databricks/python3/lib/python3.10/site-packages (from httplib2>=0.9.1->oauth2client) (3.0.9)\r\nRequirement already satisfied: charset-normalizer<3,>=2 in /databricks/python3/lib/python3.10/site-packages (from requests->curlify) (2.0.4)\r\nRequirement already satisfied: idna<4,>=2.5 in /databricks/python3/lib/python3.10/site-packages (from requests->curlify) (3.3)\r\nRequirement already satisfied: certifi>=2017.4.17 in /databricks/python3/lib/python3.10/site-packages (from requests->curlify) (2022.9.14)\r\nRequirement already satisfied: oauthlib>=3.0.0 in /usr/lib/python3/dist-packages (from requests-oauthlib>=0.7.0->google-auth-oauthlib>=0.4.1->gspread) (3.2.0)\r\nInstalling collected packages: curlify\r\nSuccessfully installed curlify-2.2.1\r\n\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m A new release of pip available: \u001B[0m\u001B[31;49m22.2.2\u001B[0m\u001B[39;49m -> \u001B[0m\u001B[32;49m23.2.1\u001B[0m\r\n\u001B[1m[\u001B[0m\u001B[34;49mnotice\u001B[0m\u001B[1;39;49m]\u001B[0m\u001B[39;49m To update, run: \u001B[0m\u001B[32;49mpip install --upgrade pip\u001B[0m\r\n"
     ]
    }
   ],
   "source": [
    "%run /Users/jakubs@activefence.com/Utils"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "0adfe335-fb35-4a29-9dfb-6b79561e3925",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [
    {
     "output_type": "display_data",
     "data": {
      "application/vnd.databricks.v1+bamboolib_hint": "{\"pd.DataFrames\": [], \"version\": \"0.0.1\"}",
      "text/plain": []
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "import requests\n",
    "from urllib.parse import urlparse\n",
    "import json\n",
    "from bs4 import BeautifulSoup\n",
    "import re\n",
    "import aiohttp\n",
    "import uuid\n",
    "\n",
    "import asyncio\n",
    "from async_timeout import timeout\n",
    "import time\n",
    "import ssl\n",
    "import certifi\n",
    "from itertools import chain\n",
    "import sys\n",
    "from dataclasses import dataclass, asdict\n",
    "from selectolax.parser import HTMLParser\n",
    "import chompjs\n",
    "from nested_lookup import nested_lookup\n",
    "from tqdm import tqdm\n",
    "\n",
    "# from google.cloud import translate_v2 as translate\n",
    "import os\n",
    "\n",
    "sys.setrecursionlimit(500)\n",
    "connector = aiohttp.TCPConnector()\n",
    "\n",
    "@dataclass\n",
    "class Pin:\n",
    "    aggregated_pin_id: str\n",
    "    pin_id: str\n",
    "    pin_url: str\n",
    "    pin_author_username: str\n",
    "    pin_author_url: str\n",
    "    source: str\n",
    "    pin_description: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PinComment:\n",
    "    aggregated_pin_id: str\n",
    "    comment_content: str\n",
    "    comment_author_url: str\n",
    "    comment_author_id: str\n",
    "    comment_author_username: str\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class PinLike:\n",
    "    pin_id: str\n",
    "    liked_by_id: str\n",
    "    liked_by_name: str\n",
    "    liked_by_url: str\n",
    "    number_of_followers: int\n",
    "\n",
    "\n",
    "@dataclass\n",
    "class Board:\n",
    "    board_id: str\n",
    "    board_url: str\n",
    "    author_id: str\n",
    "    author_name: str\n",
    "    author_url: str\n",
    "    board_title: str\n",
    "    pin_count: int\n",
    "    author_number_of_followers: int\n",
    "\n",
    "\n",
    "@dataclass(order=True)\n",
    "class SearchBoard:\n",
    "    board_id: str\n",
    "    board_url: str\n",
    "    board_title: str\n",
    "    author_id: str\n",
    "    author_name: str\n",
    "    author_url: str\n",
    "    author_number_of_followers: int\n",
    "    pin_count: int\n",
    "    search_query: int\n",
    "    search_query_translation: str\n",
    "    account_language: str\n",
    "    board_title_translation: str\n",
    "    risk_score: int\n",
    "\n",
    "@dataclass(order=True)\n",
    "class SimpleSearchBoard:\n",
    "    board_id: str\n",
    "    board_url: str\n",
    "    board_title: str\n",
    "    author_id: str\n",
    "    author_name: str\n",
    "    author_url: str\n",
    "    author_number_of_followers: int\n",
    "    pin_count: int\n",
    "    search_query: int\n",
    "    # search_query_translation: str\n",
    "    # account_language: str\n",
    "    # board_title_translation: str\n",
    "    risk_score: int\n",
    "\n",
    "@dataclass\n",
    "class Follower:\n",
    "    source_username: str\n",
    "    follower_id: str\n",
    "    follower_username: str\n",
    "    follower_name: str\n",
    "    follower_url: str\n",
    "    number_of_follower_followers: int\n",
    "    pin_count: int\n",
    "    connection_type: str"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "cdba99b3-3ee3-4181-9f5e-1b6cb78fdcbe",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def translate_text(text):\n",
    "  # try:\n",
    "  #     os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='/dbfs/FileStore/tables/grounded_burner_386311_6715ee11b915.json'\n",
    "  #     translate_client = translate.Client()\n",
    "  #     translation_results = translate_client.translate(text,\"en\")\n",
    "  #     if translation_results:\n",
    "  #       translation_results['translatedText']\n",
    "  #     else:\n",
    "  #       return\n",
    "  # except Exception as e:\n",
    "  #   print(e)\n",
    "  #   return\n",
    "  return 'Translation not turned'\n",
    "\n",
    "def detect_language(text):\n",
    "  # try:\n",
    "  #   os.environ['GOOGLE_APPLICATION_CREDENTIALS'] ='/dbfs/FileStore/tables/grounded_burner_386311_6715ee11b915.json'\n",
    "  #   translate_client = translate.Client()\n",
    "  #   decision = translate_client.detect_language(text)\n",
    "    \n",
    "  #   if not decision:\n",
    "  #     return\n",
    "\n",
    "  #   if decision['confidence'] < 0.9:\n",
    "  #     return\n",
    "  # except Exception as e:\n",
    "  #   print(e)\n",
    "  #   return \n",
    "\n",
    "  # mappings = {\n",
    "  #   \"af\": \"Afrikaans\",\n",
    "  #   \"ar\": \"Arabic\",\n",
    "  #   \"bg\": \"Bulgarian\",\n",
    "  #   \"bn\": \"Bengali\",\n",
    "  #   \"ca\": \"Catalan\",\n",
    "  #   \"cs\": \"Czech\",\n",
    "  #   \"cy\": \"Welsh\",\n",
    "  #   \"da\": \"Danish\",\n",
    "  #   \"de\": \"German\",\n",
    "  #   \"el\": \"Greek\",\n",
    "  #   \"en\": \"English\",\n",
    "  #   \"es\": \"Spanish\",\n",
    "  #   \"et\": \"Estonian\",\n",
    "  #   \"fa\": \"Persian\",\n",
    "  #   \"fi\": \"Finnish\",\n",
    "  #   \"fr\": \"French\",\n",
    "  #   \"gu\": \"Gujarati\",\n",
    "  #   \"he\": \"Hebrew\",\n",
    "  #   \"hi\": \"Hindi\",\n",
    "  #   \"hr\": \"Croatian\",\n",
    "  #   \"hu\": \"Hungarian\",\n",
    "  #   \"id\": \"Indonesian\",\n",
    "  #   \"it\": \"Italian\",\n",
    "  #   \"ja\": \"Japanese\",\n",
    "  #   \"kn\": \"Kannada\",\n",
    "  #   \"ko\": \"Korean\",\n",
    "  #   \"lt\": \"Lithuanian\",\n",
    "  #   \"lv\": \"Latvian\",\n",
    "  #   \"mk\": \"Macedonian\",\n",
    "  #   \"ml\": \"Malayalam\",\n",
    "  #   \"mr\": \"Marathi\",\n",
    "  #   \"ne\": \"Nepali\",\n",
    "  #   \"nl\": \"Dutch\",\n",
    "  #   \"no\": \"Norwegian\",\n",
    "  #   \"pa\": \"Punjabi\",\n",
    "  #   \"pl\": \"Polish\",\n",
    "  #   \"pt\": \"Portuguese\",\n",
    "  #   \"ro\": \"Romanian\",\n",
    "  #   \"ru\": \"Russian\",\n",
    "  #   \"sk\": \"Slovak\",\n",
    "  #   \"sl\": \"Slovenian\",\n",
    "  #   \"so\": \"Somali\",\n",
    "  #   \"sq\": \"Albanian\",\n",
    "  #   \"sv\": \"Swedish\",\n",
    "  #   \"sw\": \"Swahili\",\n",
    "  #   \"ta\": \"Tamil\",\n",
    "  #   \"te\": \"Telugu\",\n",
    "  #   \"th\": \"Thai\",\n",
    "  #   \"tl\": \"Tagalog\",\n",
    "  #   \"tr\": \"Turkish\",\n",
    "  #   \"uk\": \"Ukrainian\",\n",
    "  #   \"ur\": \"Urdu\",\n",
    "  #   \"vi\": \"Vietnamese\",\n",
    "  #   \"zh-cn\": \"Chinese (Simplified)\",\n",
    "  #   \"zh-tw\": \"Chinese (Traditional)\"\n",
    "  # }\n",
    "  # try: return mappings[decision['language']]\n",
    "  # except: return ''\n",
    "  return 'Detection not turned'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "6d76c5dd-38ce-47b5-8209-0fcad4009e65",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "def extract_account_name(account_url):\n",
    "    try:\n",
    "        parsed_url = urlparse(account_url)\n",
    "        username = parsed_url.path.split(\"/\")[1]\n",
    "        return username\n",
    "    except:\n",
    "        print(\"Error when extracting account\")\n",
    "        print(account_url)\n",
    "        return None\n",
    "\n",
    "def extract_pin_id(pin_url):\n",
    "    parsed_url = urlparse(pin_url)\n",
    "    pin_id = parsed_url.path.split(\"/\")[2]\n",
    "    return pin_id\n",
    "\n",
    "async def extract_board_id(board_url):\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            if 'https://' not in board_url:\n",
    "                board_url = 'https://' + board_url\n",
    "\n",
    "            sslcontext = ssl.create_default_context(cafile=certifi.where())\n",
    "            timeout = aiohttp.ClientTimeout(total=10, connect=None, sock_connect=None, sock_read=None)\n",
    "            async with aiohttp.ClientSession(trust_env=True, timeout=timeout) as session:\n",
    "                async with session.get(board_url, proxy=\"http://Hasamba:jrdy6mtgMbVDEobe4R@pr.oxylabs.io:7777\",\n",
    "                        ssl = sslcontext) as response:\n",
    "                    if not response.ok:\n",
    "                        returne\n",
    "                    \n",
    "                    html = HTMLParser(await response.text())\n",
    "                    data = html.css(\"script[type='application/json']#__PWS_DATA__\")\n",
    "                    for script in data:\n",
    "                        try:\n",
    "                            return list(nested_lookup('boards', chompjs.parse_js_object(script.text()))[0].keys())[0]\n",
    "                        except:\n",
    "                          pass\n",
    "        except Exception as e:\n",
    "            print(f' Problem with extract_board_id():', e)\n",
    "            continue\n",
    "    return \n",
    "\n",
    "async def get_pins(item_url, is_board=False, pass_board_ids=False):\n",
    "    try:\n",
    "        pins = []\n",
    "        \n",
    "        if is_board == False:\n",
    "            account_name = extract_account_name(item_url)\n",
    "            main_url = \"https://pinterest.com/resource/UserActivityPinsResource/get/\"\n",
    "            print(f\"[*] Scraping {account_name} pins\")\n",
    "            g_identifier = account_name\n",
    "        else:\n",
    "            board_url = item_url\n",
    "            main_url = \"https://pinterest.com/resource/BoardFeedResource/get/\"\n",
    "            print(f\"[*] Scraping {board_url} pins\")\n",
    "            if pass_board_ids == False:\n",
    "                try:\n",
    "                    board_id = await extract_board_id(board_url)\n",
    "                    if board_id == None:\n",
    "                      return []\n",
    "                except Exception as e:\n",
    "                    print(e)\n",
    "                    print(f\"[*] Can't extract board ID for {board_url}\")\n",
    "                    return pins\n",
    "            else:\n",
    "                board_id = item_url\n",
    "            g_identifier = board_id\n",
    "    \n",
    "        async def make_request(identifier, is_board, pass_board_ids, bookmark=None):\n",
    "\n",
    "            if is_board == False:\n",
    "                querystring = {\n",
    "                    \"data\": '{\"options\":{\"data\":{},\"exclude_add_pin_rep\":true,\"field_set_key\":\"grid_item\",\"is_own_profile_pins\":false,\"redux_normalize_feed\":true,\"username\":\"onekindesign\"},\"context\":{}}',\n",
    "                }\n",
    "\n",
    "                querystring_dict = json.loads(querystring[\"data\"])\n",
    "                querystring_dict[\"options\"][\"username\"] = identifier\n",
    "            else:\n",
    "                querystring = {\n",
    "                    \"data\": '{\"options\":{\"board_id\":\"654499827036738988\",\"currentFilter\":-1,\"field_set_key\":\"react_grid_pin\",\"filter_section_pins\":true,\"sort\":\"default\",\"layout\":\"default\",\"page_size\":25,\"redux_normalize_feed\":true}}',\n",
    "                }\n",
    "\n",
    "                querystring_dict = json.loads(querystring[\"data\"])\n",
    "                querystring_dict[\"options\"][\"board_id\"] = identifier\n",
    "            \n",
    "            if len(pins) >= 500:\n",
    "                print(\"[*] More than 500 pins. Not collecting anymore.\")\n",
    "                return \n",
    "            \n",
    "            if bookmark is not None:\n",
    "                querystring_dict[\"options\"][\"bookmarks\"] = [bookmark]\n",
    "                print(f\"[*] Loading new pins {bookmark}\")\n",
    "            querystring = {\"data\": json.dumps(querystring_dict)}\n",
    "          \n",
    "            \n",
    "            sslcontext = ssl.create_default_context(cafile=certifi.where())\n",
    "            timeout = aiohttp.ClientTimeout(total=10, connect=None, sock_connect=None, sock_read=None)\n",
    "            async with aiohttp.ClientSession(trust_env=True, timeout=timeout) as session:\n",
    "                async with session.get(main_url, params=querystring, proxy=\"http://Hasamba:jrdy6mtgMbVDEobe4R@pr.oxylabs.io:7777\",\n",
    "                        ssl = sslcontext) as response:\n",
    "                    if not response.ok:\n",
    "                        return\n",
    "              \n",
    "                    response = await response.json()\n",
    "                    if is_board:\n",
    "                        items = response[\"resource_response\"][\"data\"][:-1]\n",
    "                    else:\n",
    "                        items = response[\"resource_response\"][\"data\"]\n",
    "\n",
    "                    for item in items:\n",
    "                        try:\n",
    "                            pins.append(\n",
    "                                Pin(\n",
    "                                    aggregated_pin_id=item[\"aggregated_pin_data\"][\"id\"],\n",
    "                                    pin_id=item[\"id\"],\n",
    "                                    pin_url=f'https://pinterest.com/pin/{item[\"id\"]}/',\n",
    "                                    pin_author_username = item['pinner']['username'],\n",
    "                                    pin_author_url=f\"https://pinterest.com/{item['pinner']['username']}/\",\n",
    "                                    source= item['board']['url'],\n",
    "                                    pin_description= item['description'],\n",
    "                                )\n",
    "                            )\n",
    "                            print(f'[*] Scraped -> https://pinterest.com/pin/{item[\"id\"]}/')\n",
    "                        except:\n",
    "                            print(\n",
    "                                f'[!] Problem with scraping -> https://pinterest.com/pin/{item[\"id\"]}/'\n",
    "                            )\n",
    "\n",
    "                    if not response.get(\"resource_response\").get(\"bookmark\"):\n",
    "                        return\n",
    "\n",
    "                    return await make_request(\n",
    "                        identifier, is_board, pass_board_ids, response[\"resource_response\"][\"bookmark\"]\n",
    "                    )\n",
    "\n",
    "        await make_request(g_identifier, is_board, pass_board_ids)\n",
    "        print(f\"[*] Finished scraping {g_identifier} pins (found {len(pins)} pins)\")\n",
    "        return pins\n",
    "    except Exception as e:\n",
    "      # print(e)\n",
    "      return [] \n",
    "\n",
    "async def get_boards(account_name):\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            # print(f\"[*] Scraping {account_name} boards\")\n",
    "\n",
    "            boards = []\n",
    "\n",
    "            main_url = \"https://br.pinterest.com/resource/BoardsResource/get/\"\n",
    "\n",
    "            async def make_request(account_name, bookmark=None):\n",
    "\n",
    "                querystring = {\n",
    "                    \"data\": '{\"options\":{\"privacy_filter\":\"all\",\"sort\":\"last_pinned_to\",\"field_set_key\":\"profile_grid_item\",\"filter_stories\":false,\"username\":\"sergeritopop\",\"page_size\":25,\"group_by\":\"visibility\",\"include_archived\":true,\"redux_normalize_feed\":true}}',\n",
    "                }\n",
    "\n",
    "                querystring_dict = json.loads(querystring[\"data\"])\n",
    "                querystring_dict[\"options\"][\"username\"] = account_name\n",
    "\n",
    "                if bookmark != None:\n",
    "                    querystring_dict[\"options\"][\"bookmarks\"] = [bookmark]\n",
    "                    # print(f\"[*] Loading new boards {bookmark}\")\n",
    "                querystring = {\"data\": json.dumps(querystring_dict)}\n",
    "\n",
    "                # Stop scraping if scraped moe than 100 boards per user\n",
    "                if len(boards) >= 100:\n",
    "                    return\n",
    "                  \n",
    "                sslcontext = ssl.create_default_context(cafile=certifi.where())\n",
    "                timeout = aiohttp.ClientTimeout(total=30, connect=None, sock_connect=None, sock_read=None)\n",
    "                async with aiohttp.ClientSession(trust_env=True, timeout=timeout) as session:\n",
    "                    async with session.get(main_url, params=querystring, proxy=\"http://Hasamba:jrdy6mtgMbVDEobe4R@pr.oxylabs.io:7777\",\n",
    "                            ssl = sslcontext) as response:\n",
    "                        if not response.ok:\n",
    "                            return\n",
    "\n",
    "                        response = await response.json()\n",
    "                        for item in response[\"resource_response\"][\"data\"][1:]:\n",
    "                            try:\n",
    "                                boards.append(\n",
    "                                    Board(\n",
    "                                        board_id = item[\"id\"],\n",
    "                                        board_url=f\"https://pinterest.com{item['url']}\",\n",
    "                                        board_title=item[\"name\"],\n",
    "                                        author_id=item[\"owner\"][\"id\"],\n",
    "                                        author_name=item['owner']['full_name'],\n",
    "                                        author_url=f\"https://pinterest.com/{item['owner']['username']}/\",\n",
    "                                        pin_count=item[\"pin_count\"],\n",
    "                                        author_number_of_followers=(await get_account_data(\n",
    "                                            item[\"owner\"][\"username\"]\n",
    "                                        ))[\"resource_response\"][\"data\"][\"follower_count\"],\n",
    "                                    ),\n",
    "                                )\n",
    "                                # print(f\"[*] Scraped -> https://pinterest.com{item['url']}\")\n",
    "                            except Exception as e:\n",
    "                                print(e)\n",
    "                                continue\n",
    "                        if not response.get(\"resource_response\").get(\"bookmark\"):\n",
    "                            return\n",
    "\n",
    "                        return await make_request(\n",
    "                            account_name, response[\"resource_response\"][\"bookmark\"]\n",
    "                        )\n",
    "\n",
    "            await make_request(account_name)\n",
    "            # print(f\"[*] Finished scraping {account_name} boards (found {len(boards)} boards)\")\n",
    "            return boards\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            continue\n",
    "    return []\n",
    "\n",
    "async def get_followers(account_name):\n",
    "\n",
    "    print(f\"[*] Scraping {account_name} followers\")\n",
    "\n",
    "    account_id = (await get_account_data(account_name))[\"resource_response\"][\"data\"][\"id\"]\n",
    "    followers = []\n",
    "\n",
    "    if account_id is None:\n",
    "        return followers\n",
    "\n",
    "    main_url = \"https://pl.pinterest.com/_/graphql/\"\n",
    "\n",
    "    async def make_request(account_id, cursor=None, id=None):\n",
    "\n",
    "        cookies = {\n",
    "            \"csrftoken\": \"1bf86076468783ac8e0bc10afd3e311d\",\n",
    "        }\n",
    "\n",
    "        headers = {\n",
    "            \"x-csrftoken\": \"1bf86076468783ac8e0bc10afd3e311d\",\n",
    "        }\n",
    "\n",
    "        json_data = {\n",
    "            \"queryHash\": \"113a04938a3424313cd86088ca2f07ea7750d863589ede601cba4e731ea68ea2\",\n",
    "            \"variables\": {\n",
    "                \"userId\": \"490329615588402909\",\n",
    "            },\n",
    "        }\n",
    "\n",
    "        json_data[\"variables\"][\"userId\"] = str(account_id)\n",
    "\n",
    "        if cursor != None:\n",
    "            json_data = {\n",
    "                \"queryHash\": \"ac0d278d416e5fc59ee8124e9e7402efdfa1dea7a952a61e5c31926e70586b1b\",\n",
    "                \"variables\": {\n",
    "                    \"count\": 10,\n",
    "                    \"cursor\": \"MjIyMzo4OTc2MjM5MDY4ODg4MDM5Njg6OTIyMzM3MDM2MzM3NzU5NTEzN19F\",\n",
    "                    \"id\": \"VXNlcjo0MjM0NzkzMDg2MTk2NDc3MDA=\",\n",
    "                },\n",
    "            }\n",
    "\n",
    "            json_data[\"variables\"][\"cursor\"] = str(cursor)\n",
    "            json_data[\"variables\"][\"id\"] = str(id)\n",
    "            print(f\"[*] Loading new followers {cursor}\")\n",
    "\n",
    "        # Stop scraping if scraped moe than 100 followers per user\n",
    "        # if len(followers) >= 100:\n",
    "        #     return\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.post(\n",
    "                main_url, json=json_data, cookies=cookies, headers=headers, proxy=\"http://Hasamba:jrdy6mtgMbVDEobe4R@pr.oxylabs.io:7777\",\n",
    "            ) as response:\n",
    "                if not response.ok:\n",
    "                    return\n",
    "                response = await response.json()\n",
    "\n",
    "                if \"v3GetUserHandlerQuery\" in response[\"data\"]:\n",
    "                    for item in response[\"data\"][\"v3GetUserHandlerQuery\"][\"data\"][\n",
    "                        \"followers\"\n",
    "                    ][\"connection\"][\"edges\"]:\n",
    "                        try:\n",
    "                            followers_data = await get_account_data(item[\"node\"][\"username\"])\n",
    "                            followers.append(\n",
    "                                Follower(\n",
    "                                    source_username=account_name,\n",
    "                                    follower_id=item[\"node\"][\"entityId\"],\n",
    "                                    follower_username=item[\"node\"][\"username\"],\n",
    "                                    follower_name=item[\"node\"][\"fullName\"],\n",
    "                                    follower_url=f\"https://pinterest.com/{item['node']['username']}/\",\n",
    "                                    number_of_follower_followers= followers_data[\"resource_response\"][\"data\"][\"follower_count\"],\n",
    "                                    pin_count = followers_data[\"resource_response\"]['data']['pin_count'],\n",
    "                                    connection_type = \"Follower\",\n",
    "                                )\n",
    "                            )\n",
    "                            print(\n",
    "                                f\"[*] Scraped -> https://pinterest.com/{item['node']['username']}/\"\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "\n",
    "                    if (\n",
    "                        response[\"data\"][\"v3GetUserHandlerQuery\"][\"data\"][\"followers\"][\n",
    "                            \"connection\"\n",
    "                        ][\"pageInfo\"][\"hasNextPage\"]\n",
    "                        == False\n",
    "                    ):\n",
    "                        return\n",
    "\n",
    "                    return await make_request(\n",
    "                        account_id,\n",
    "                        response[\"data\"][\"v3GetUserHandlerQuery\"][\"data\"][\"followers\"][\n",
    "                            \"connection\"\n",
    "                        ][\"pageInfo\"][\"endCursor\"],\n",
    "                        response[\"data\"][\"v3GetUserHandlerQuery\"][\"data\"][\"id\"],\n",
    "                    )\n",
    "                else:\n",
    "                    for item in response[\"data\"][\"node\"][\"followers\"][\"connection\"][\n",
    "                        \"edges\"\n",
    "                    ]:\n",
    "                        try:\n",
    "                            followers_data = await get_account_data(item[\"node\"][\"username\"])\n",
    "                            followers.append(\n",
    "                                Follower(\n",
    "                                    source_username=account_name,\n",
    "                                    follower_id=item[\"node\"][\"entityId\"],\n",
    "                                    follower_username=item[\"node\"][\"username\"],\n",
    "                                    follower_name=item[\"node\"][\"fullName\"],\n",
    "                                    follower_url=f\"https://pinterest.com/{item['node']['username']}/\",\n",
    "                                    number_of_follower_followers= followers_data[\"resource_response\"][\"data\"][\"follower_count\"],\n",
    "                                    pin_count = followers_data[\"resource_response\"]['data']['pin_count'],\n",
    "                                    connection_type = \"Follower\",\n",
    "                                ),\n",
    "                            )\n",
    "                            print(\n",
    "                                f\"[*] Scraped -> https://pinterest.com/{item['node']['username']}/\"\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "                            continue\n",
    "\n",
    "                    if (\n",
    "                        response[\"data\"][\"node\"][\"followers\"][\"connection\"][\"pageInfo\"][\n",
    "                            \"hasNextPage\"\n",
    "                        ]\n",
    "                        == False\n",
    "                    ):\n",
    "                        return\n",
    "\n",
    "                    return await make_request(\n",
    "                        account_id,\n",
    "                        response[\"data\"][\"node\"][\"followers\"][\"connection\"][\"pageInfo\"][\n",
    "                            \"endCursor\"\n",
    "                        ],\n",
    "                        response[\"data\"][\"node\"][\"id\"],\n",
    "                    )\n",
    "\n",
    "    await make_request(account_id)\n",
    "    print(\n",
    "        f\"[*] Finished scraping {account_name} followers (found {len(followers)} followers)\"\n",
    "    )\n",
    "    return followers\n",
    "\n",
    "\n",
    "async def get_board_followers(board_url, _pinterest_sess):\n",
    "    try:\n",
    "        board_followers = []\n",
    "\n",
    "        main_url = \"https://za.pinterest.com/resource/BoardFollowersResource/get/\"\n",
    "\n",
    "        headers = {\"cookie\": f\"_pinterest_sess={_pinterest_sess};\"}\n",
    "        \n",
    "        try:\n",
    "            board_id = await extract_board_id(board_url)\n",
    "        except Exception as e:\n",
    "            print(e)\n",
    "            print(f\"[*] Can't extract board ID for {board_url}\")\n",
    "\n",
    "        async def make_request(board_id, _pinterest_sess, bookmark=None):\n",
    "            querystring = {\"data\":\"{\\\"options\\\":{\\\"board_id\\\":\\\"808537008040347095\\\",\\\"page_size\\\":50}}\"}\n",
    "            querystring_dict = json.loads(querystring[\"data\"])\n",
    "            querystring_dict[\"options\"][\"options\"] = board_id\n",
    "\n",
    "            if bookmark != None:\n",
    "                querystring_dict[\"options\"][\"bookmarks\"] = [bookmark]\n",
    "            querystring = {\"data\": json.dumps(querystring_dict)}\n",
    "\n",
    "            if len(board_followers) >= 1000:\n",
    "                return\n",
    "\n",
    "            async with aiohttp.ClientSession() as session:\n",
    "                async with session.get(\n",
    "                    main_url, params=querystring, headers=headers\n",
    "                ) as response:\n",
    "                    if not response.ok:\n",
    "                        return\n",
    "                    response = await response.json()\n",
    "                    for item in response[\"resource_response\"][\"data\"]:\n",
    "                        try:\n",
    "                            board_followers.append(\n",
    "                                    Follower(\n",
    "                                        source_username=board_url,\n",
    "                                        follower_id=item[\"id\"],\n",
    "                                        follower_username=item[\"username\"],\n",
    "                                        follower_name=item[\"full_name\"],\n",
    "                                        follower_url=f\"https://pinterest.com/{item['username']}/\",\n",
    "                                        number_of_follower_followers=(\n",
    "                                            await get_account_data(item[\"username\"])\n",
    "                                        )[\"resource_response\"][\"data\"][\"follower_count\"],\n",
    "                                    )\n",
    "                            )\n",
    "                        except Exception as e:\n",
    "                            print(e)\n",
    "\n",
    "                    if not response.get(\"resource_response\").get(\"bookmark\"):\n",
    "                        return\n",
    "\n",
    "                    await make_request(\n",
    "                        board_id,\n",
    "                        _pinterest_sess,\n",
    "                        response[\"resource_response\"][\"bookmark\"],\n",
    "                    )\n",
    "\n",
    "        await make_request(board_id, _pinterest_sess)\n",
    "        print(\n",
    "            f\"[*] Finished scraping followees {board_id} (found {len(board_followers)} followees)\"\n",
    "        )\n",
    "        return board_followers\n",
    "    except:\n",
    "        return None\n",
    "\n",
    "async def get_pin_likes(pin_id, _pinterest_sess):\n",
    "    likes = []\n",
    "    for i in range(3):\n",
    "      try:\n",
    "          \n",
    "\n",
    "          main_url = \"https://pl.pinterest.com/resource/ReactionsResource/get/\"\n",
    "\n",
    "          headers = {\"cookie\": f\"_pinterest_sess={_pinterest_sess};\"}\n",
    "          \n",
    "          async def make_request(pin_id, _pinterest_sess, bookmark=None):\n",
    "              querystring = {\n",
    "                  \"data\": '{\"options\":{\"pin_id\":\"719168634263865371\"}}',\n",
    "              }\n",
    "              querystring_dict = json.loads(querystring[\"data\"])\n",
    "              querystring_dict[\"options\"][\"pin_id\"] = pin_id\n",
    "\n",
    "              if bookmark != None:\n",
    "                  querystring_dict[\"options\"][\"bookmarks\"] = [bookmark]\n",
    "                  # print(f'[*] Loading new likes {bookmark}')\n",
    "              querystring = {\"data\": json.dumps(querystring_dict)}\n",
    "\n",
    "              if len(likes) >= 200:\n",
    "                  return\n",
    "\n",
    "              sslcontext = ssl.create_default_context(cafile=certifi.where())\n",
    "              async with aiohttp.ClientSession(trust_env=True) as session:\n",
    "                  async with session.get(\n",
    "                      main_url, params=querystring, headers=headers, ssl = sslcontext, proxy=\"http://Hasamba:jrdy6mtgMbVDEobe4R@pr.oxylabs.io:7777\",\n",
    "                  ) as response:\n",
    "                      if not response.ok:\n",
    "                          return\n",
    "                        \n",
    "                      response = await response.json()\n",
    "                      for item in response[\"resource_response\"][\"data\"]:\n",
    "                          try:\n",
    "                              likes.append(\n",
    "                                  PinLike(\n",
    "                                      pin_id=pin_id,\n",
    "                                      liked_by_id=item[\"user\"][\"id\"],\n",
    "                                      liked_by_name=item[\"user\"][\"username\"],\n",
    "                                      liked_by_url=f\"https://pinterest.com/{item['user']['username']}/\",\n",
    "                                      number_of_followers=(await\n",
    "                                          get_account_data(item[\"user\"][\"username\"]))[\n",
    "                                              \"resource_response\"\n",
    "                                          ][\"data\"][\"follower_count\"],\n",
    "                                  )\n",
    "                              )\n",
    "                              print(f\"[*] Scraped -> https://pinterest.com/{item['user']['username']}/\")\n",
    "                          except Exception as e:\n",
    "                              print(e)\n",
    "\n",
    "                      if not response.get(\"resource_response\").get(\"bookmark\"):\n",
    "                          return\n",
    "\n",
    "                      await make_request(\n",
    "                          pin_id,\n",
    "                          _pinterest_sess,\n",
    "                          response[\"resource_response\"][\"bookmark\"],\n",
    "                      )\n",
    "\n",
    "          await make_request(pin_id, _pinterest_sess)\n",
    "          print(\n",
    "              f\"[*] Finished scraping likes for pin {pin_id} (found {len(likes)} likes)\"\n",
    "          )\n",
    "          return likes\n",
    "      # Server disconnected as explained here: https://stackoverflow.com/questions/55853195/getting-serverdisconnectederror-exceptions-would-connection-release-help-fix\n",
    "      except aiohttp.ClientConnectionError:\n",
    "            print(\"[*] Oops, the connection was dropped before we finished: https://stackoverflow.com/questions/55853195/getting-serverdisconnectederror-exceptions-would-connection-release-help-fix.\")\n",
    "            continue\n",
    "      except Exception as e:\n",
    "          print(e)\n",
    "          continue\n",
    "    return []\n",
    "      \n",
    "async def get_pin_comments(aggregated_pin_id):\n",
    "    comments = []\n",
    "    for i in range(3):\n",
    "      try:\n",
    "          \n",
    "          main_url = \"https://pl.pinterest.com/resource/UnifiedCommentsResource/get/\"\n",
    "\n",
    "          async def make_request(account_name, bookmark=None):\n",
    "              querystring = {\n",
    "                  \"data\": '{\"options\":{\"aggregated_pin_id\":\"5201253337757958901\",\"comment_featured_ids\":\"\",\"page_size\":5,\"redux_normalize_feed\":true},\"context\":{}}',\n",
    "              }\n",
    "              querystring_dict = json.loads(querystring[\"data\"])\n",
    "              querystring_dict[\"options\"][\"aggregated_pin_id\"] = aggregated_pin_id\n",
    "              \n",
    "              if len(comments) >= 200:\n",
    "                  return\n",
    "\n",
    "              if bookmark != None:\n",
    "                  querystring_dict[\"options\"][\"bookmarks\"] = [bookmark]\n",
    "                  print(f\"[*] Loading new comments {bookmark}\")\n",
    "              querystring = {\"data\": json.dumps(querystring_dict)}\n",
    "\n",
    "              sslcontext = ssl.create_default_context(cafile=certifi.where())              \n",
    "              async with aiohttp.ClientSession(trust_env=True) as session:\n",
    "                  async with session.get(\n",
    "                      main_url,\n",
    "                      params=querystring,\n",
    "                      proxy=\"http://Hasamba:jrdy6mtgMbVDEobe4R@pr.oxylabs.io:7777\",\n",
    "                      ssl = sslcontext,\n",
    "                  ) as response:\n",
    "                      if not response.ok:\n",
    "                          return\n",
    "\n",
    "                      response = await response.json()\n",
    "                      for item in response[\"resource_response\"][\"data\"]:\n",
    "                          try:\n",
    "                              comments.append(\n",
    "                                  PinComment(\n",
    "                                      aggregated_pin_id=aggregated_pin_id,\n",
    "                                      comment_content=item[\"text\"],\n",
    "                                      comment_author_id=item[\"user\"][\"id\"],\n",
    "                                      comment_author_url=f\"https://pinterest.com/{item['user']['username']}/\",\n",
    "                                      comment_author_username=item[\"user\"][\"username\"],\n",
    "                                  )\n",
    "                              )\n",
    "                          except Exception as e:\n",
    "                              comments.append(\n",
    "                                  PinComment(\n",
    "                                      aggregated_pin_id=aggregated_pin_id,\n",
    "                                      comment_content=\"\",\n",
    "                                      comment_author_id=item[\"user\"][\"id\"],\n",
    "                                      comment_author_url=f\"https://pinterest.com/{item['user']['username']}/\",\n",
    "                                      comment_author_username=item[\"user\"][\"username\"],\n",
    "                                  )\n",
    "                              )\n",
    "                              print('Exception:', e)\n",
    "                          print(f\"[*] Scraped -> https://pinterest.com/{item['user']['username']}/\")     \n",
    "\n",
    "                      if not response.get(\"resource_response\").get(\"bookmark\"):\n",
    "                          return\n",
    "\n",
    "                      return await make_request(\n",
    "                          aggregated_pin_id, response[\"resource_response\"][\"bookmark\"]\n",
    "                      )\n",
    "\n",
    "          await make_request(aggregated_pin_id)\n",
    "          print(\n",
    "              f\"[*] Finished scraping comments for pin {aggregated_pin_id} (found {len(comments)} comments)\"\n",
    "          )\n",
    "          return comments\n",
    "      except Exception as e:\n",
    "          print(e)\n",
    "          continue\n",
    "    return []"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "4ef382ef-4abe-47d7-a6b7-df084f449743",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "async def get_account_data(account_name):\n",
    "  for i in range(3):\n",
    "      try:\n",
    "          main_url = \"https://pl.pinterest.com/resource/UserResource/get/\"\n",
    "\n",
    "          querystring = {\n",
    "              \"data\": '{\"options\":{\"username\":\"4SPORTS97\",\"field_set_key\":\"profile\"}}',\n",
    "          }\n",
    "          querystring_dict = json.loads(querystring[\"data\"])\n",
    "          querystring_dict[\"options\"][\"username\"] = account_name\n",
    "          querystring = {\"data\": json.dumps(querystring_dict)}\n",
    "\n",
    "          sslcontext = ssl.create_default_context(cafile=certifi.where())\n",
    "          timeout = aiohttp.ClientTimeout(total=30, connect=None, sock_connect=None, sock_read=None)\n",
    "          async with aiohttp.ClientSession(trust_env=True, timeout=timeout) as session:\n",
    "              async with session.get(main_url, params=querystring, proxy=\"http://Hasamba:jrdy6mtgMbVDEobe4R@pr.oxylabs.io:7777\",\n",
    "                      ssl = sslcontext) as response:\n",
    "                  if not response.ok:\n",
    "                      return\n",
    "                  response = await response.json()\n",
    "                  return response\n",
    "      except Exception as e:\n",
    "          print(e)\n",
    "          continue\n",
    "  return  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "7121e8e8-216d-4bae-91a7-62b828fe7a16",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "async def scrape_comments(aggregated_pin_ids, batch_size=50):\n",
    "    random_id = uuid.uuid1()\n",
    "    print(f's3://activefence-user/jakubs/pinterest/comments-{random_id}')\n",
    "    time.sleep(3)\n",
    "\n",
    "    columns = ['aggregated_pin_id', 'pin_id', 'pin_url', 'comment_author_id','comment_author_url','comment_author_username','comment_content']\n",
    "\n",
    "    iterator = 1\n",
    "    for i in range(0, len(aggregated_pin_ids), batch_size):\n",
    "        batch = aggregated_pin_ids[i : i + batch_size]\n",
    "        tasks = [get_pin_comments(aggregated_pin_id) for aggregated_pin_id in batch]\n",
    "        batch_results = await asyncio.gather(*tasks)\n",
    "        batch_results = list(chain(*batch_results))\n",
    "\n",
    "        print(f'[*] Scraped {len(batch_results)} comments in batch #{iterator} / out of #{len(range(0, len(aggregated_pin_ids), batch_size))}')\n",
    "        \n",
    "        if len(batch_results) == 0:\n",
    "            continue\n",
    "          \n",
    "        # As here: https://delta.io/blog/2022-11-01-pyspark-save-mode-append-overwrite-error/ \n",
    "        rdd = spark.sparkContext.parallelize(batch_results)\n",
    "        batch_results_df = rdd.toDF(columns)\n",
    "        batch_results_df.repartition(100).write.mode('append').format(\"parquet\").save(f's3://activefence-user/jakubs/pinterest/comments-{random_id}')\n",
    "\n",
    "        # Used for storing information about the current batch\n",
    "        iterator += 1\n",
    "    return f's3://activefence-user/jakubs/pinterest/comments-{random_id}'\n",
    "\n",
    "async def scrape_likes(pin_ids, _pinterest_sess, batch_size=50):\n",
    "    random_id = uuid.uuid1()\n",
    "    print(f's3://activefence-user/jakubs/pinterest/likes-{random_id}')\n",
    "    time.sleep(3)\n",
    "\n",
    "    columns = ['pin_id', 'liked_by_id','liked_by_name','liked_by_url','number_of_followers']\n",
    "    \n",
    "    iterator = 1\n",
    "    for i in range(0, len(pin_ids), batch_size):\n",
    "        batch = pin_ids[i : i + batch_size]\n",
    "        tasks = [get_pin_likes(pin_id, _pinterest_sess) for pin_id in batch]\n",
    "        batch_results = await asyncio.gather(*tasks)\n",
    "        batch_results = list(chain(*batch_results))\n",
    "\n",
    "        print(f'[*] Scraped {len(batch_results)} likes in batch #{iterator} / out of #{len(range(0, len(pin_ids), batch_size))}')\n",
    "\n",
    "        if len(batch_results) == 0:\n",
    "            continue\n",
    "\n",
    "        rdd = spark.sparkContext.parallelize(batch_results)\n",
    "        batch_results_df = rdd.toDF(columns)\n",
    "        batch_results_df.repartition(100).write.mode('append').format(\"parquet\").save(f's3://activefence-user/jakubs/pinterest/likes-{random_id}')\n",
    "\n",
    "        # Used for storing information about the current batch\n",
    "        iterator += 1\n",
    "    return f's3://activefence-user/jakubs/pinterest/likes-{random_id}'\n",
    "\n",
    "async def scrape_boards(account_names, batch_size=50, specific_save_path=None):\n",
    "    random_id = uuid.uuid1()\n",
    "    print(f's3://activefence-user/jakubs/pinterest/boards-{random_id}')\n",
    "    time.sleep(3)\n",
    "\n",
    "    columns = ['author_id', 'author_number_of_followers', 'author_url', 'author_name', 'board_id', 'board_title', 'board_url', 'pin_count']\n",
    "\n",
    "    iterator = 1\n",
    "    for i in tqdm(range(0, len(account_names), batch_size)):\n",
    "        batch = account_names[i : i + batch_size]\n",
    "        print('[e] Starting tasks.')\n",
    "        tasks = [get_boards(account_name) for account_name in batch]\n",
    "        print('[e] Ending tasks - gathering them.')\n",
    "        batch_results = await asyncio.gather(*tasks)\n",
    "        print('[e] Chain on batch results')\n",
    "        batch_results = list(chain(*batch_results))\n",
    "\n",
    "        print(f'[*] Scraped {len(batch_results)} boards in batch #{iterator} / out of #{len(range(0, len(account_names), batch_size))}')\n",
    "        \n",
    "        if len(batch_results) == 0:\n",
    "            continue\n",
    "          \n",
    "        # As here: https://delta.io/blog/2022-11-01-pyspark-save-mode-append-overwrite-error/ \n",
    "        rdd = spark.sparkContext.parallelize(batch_results)\n",
    "        print('[e] End to parallelize - rdd')\n",
    "\n",
    "        batch_results_df = rdd.toDF(columns)\n",
    "        print('[e] Starting repartition ')\n",
    "\n",
    "        if specific_save_path is None:\n",
    "            batch_results_df.repartition(1000).write.mode('append').format(\"parquet\").save(f's3://activefence-user/jakubs/pinterest/boards-{random_id}')\n",
    "        else: \n",
    "            batch_results_df.repartition(1000).write.mode('append').format(\"parquet\").save(specific_save_path)\n",
    "\n",
    "        # Used for storing information about the current batch\n",
    "        iterator += 1\n",
    "    return f's3://activefence-user/jakubs/pinterest/boards-{random_id}'\n",
    "\n",
    "async def scrape_pins(item_urls, is_board, pass_board_ids, batch_size=50, specific_save_path=None):\n",
    "    random_id = uuid.uuid1()\n",
    "    print(f's3://activefence-user/jakubs/pinterest/pins-{random_id}')\n",
    "    time.sleep(3)\n",
    "\n",
    "    columns = ['aggregated_pin_id', 'pin_id', 'pin_url', 'pin_author_username', 'pin_author_url','source', 'pin_description']\n",
    "\n",
    "    iterator = 1\n",
    "    for i in range(0, len(item_urls), batch_size):\n",
    "        batch = item_urls[i : i + batch_size]\n",
    "        tasks = [get_pins(item_url, is_board, pass_board_ids) for item_url in batch]\n",
    "        batch_results = await asyncio.gather(*tasks)\n",
    "        batch_results = list(chain(*batch_results))\n",
    "\n",
    "        print(f'[*] Scraped {len(batch_results)} pins in batch #{iterator} / out of #{len(range(0, len(item_urls), batch_size))}')\n",
    "        \n",
    "        if len(batch_results) == 0:\n",
    "            continue\n",
    "          \n",
    "        rdd = spark.sparkContext.parallelize(batch_results)\n",
    "        batch_results_df = rdd.toDF(columns)\n",
    "        if specific_save_path is None:\n",
    "            batch_results_df.repartition(1000).write.mode('append').format(\"parquet\").save(f's3://activefence-user/jakubs/pinterest/pins-{random_id}')\n",
    "        else: \n",
    "            batch_results_df.repartition(1000).write.mode('append').format(\"parquet\").save(specific_save_path)\n",
    "\n",
    "        # Used for storing information about the current batch\n",
    "        iterator += 1\n",
    "    return f's3://activefence-user/jakubs/pinterest/pins-{random_id}'\n",
    "\n",
    "import traceback\n",
    "\n",
    "async def scrape_followees(account_names, _pinterest_sess, batch_size=20, specific_save_path=None):\n",
    "    random_id = uuid.uuid1()\n",
    "    print(f's3://activefence-user/jakubs/pinterest/followees-{random_id}')\n",
    "\n",
    "    # columns = ['author_id', 'followee_id', 'followee_url', 'followee_name']\n",
    "\n",
    "    iterator = 1\n",
    "    for i in tqdm(range(0, len(account_names), batch_size)):\n",
    "        try:\n",
    "            batch = account_names[i : i + batch_size]\n",
    "            tasks = [get_followees(account_name, _pinterest_sess) for account_name in batch]\n",
    "\n",
    "            batch_results = await asyncio.gather(*tasks)\n",
    "            # batch_results = [result for result in batch_results if not isinstance(result, Exception)]\n",
    "            batch_results = list(chain(*batch_results))\n",
    "            print(f'[*] Scraped {len(batch_results)} followees in batch #{iterator} / out of #{len(range(0, len(account_names), batch_size))}')\n",
    "\n",
    "            if len(batch_results) == 0:\n",
    "                continue\n",
    "\n",
    "            rdd = spark.sparkContext.parallelize(batch_results)\n",
    "            batch_results_df = rdd.toDF()\n",
    "\n",
    "            if specific_save_path is None:\n",
    "                batch_results_df.repartition(10).write.mode('append').format(\"parquet\").save(f's3://activefence-user/jakubs/pinterest/followees-{random_id}')\n",
    "                print('Write success.')\n",
    "            else: \n",
    "                batch_results_df.repartition(10).write.mode('append').format(\"parquet\").save(specific_save_path)\n",
    "            \n",
    "            iterator += 1\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during the processing of batch #{iterator}\")\n",
    "            print(traceback.format_exc())\n",
    "            continue\n",
    "\n",
    "    return f's3://activefence-user/jakubs/pinterest/followees-{random_id}'\n",
    "\n",
    "\n",
    "async def scrape_followers(account_names, batch_size=20, specific_save_path=None):\n",
    "    random_id = uuid.uuid1()\n",
    "    print(f's3://activefence-user/jakubs/pinterest/followers-{random_id}')\n",
    "\n",
    "    # columns = ['author_id', 'followee_id', 'followee_url', 'followee_name']\n",
    "\n",
    "    iterator = 1\n",
    "    for i in tqdm(range(0, len(account_names), batch_size)):\n",
    "        try:\n",
    "            batch = account_names[i : i + batch_size]\n",
    "            tasks = [get_followers(account_name) for account_name in batch]\n",
    "\n",
    "            batch_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            batch_results = [result for result in batch_results if not isinstance(result, Exception)]\n",
    "            batch_results = list(chain(*batch_results))\n",
    "            print(f'[*] Scraped {len(batch_results)} followers in batch #{iterator} / out of #{len(range(0, len(account_names), batch_size))}')\n",
    "\n",
    "            if len(batch_results) == 0:\n",
    "                continue\n",
    "\n",
    "            rdd = spark.sparkContext.parallelize(batch_results)\n",
    "            batch_results_df = rdd.toDF()\n",
    "\n",
    "            if specific_save_path is None:\n",
    "                batch_results_df.repartition(10).write.mode('append').format(\"parquet\").save(f's3://activefence-user/jakubs/pinterest/followers-{random_id}')\n",
    "            else: \n",
    "                batch_results_df.repartition(10).write.mode('append').format(\"parquet\").save(specific_save_path)\n",
    "            \n",
    "            iterator += 1\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during the processing of batch #{iterator}\")\n",
    "            print(traceback.format_exc())\n",
    "            continue\n",
    "\n",
    "    return f's3://activefence-user/jakubs/pinterest/followers-{random_id}'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 0,
   "metadata": {
    "application/vnd.databricks.v1+cell": {
     "cellMetadata": {
      "byteLimit": 2048000,
      "rowLimit": 10000
     },
     "inputWidgets": {},
     "nuid": "10ca3b88-75be-40ab-9867-b0703be58f6e",
     "showTitle": false,
     "title": ""
    }
   },
   "outputs": [],
   "source": [
    "\n",
    "kw_sheet = load_sheet(\"[INT] Pinterest - Feeds\", 1056641898)\n",
    "kw_df = pd.DataFrame(kw_sheet.get_values())\n",
    "kw_df.columns = kw_df.iloc[0]\n",
    "kw_df = kw_df[1:]\n",
    "kw_values = kw_df.values.tolist()\n",
    "\n",
    "async def get_search_result_boards(query, pinterest_sess):\n",
    "    boards = []\n",
    "    for i in range(3):\n",
    "        try:\n",
    "            print(f\"[*] Scraping {query} boards\")\n",
    "\n",
    "            main_url = \"https://pl.pinterest.com/resource/BaseSearchResource/get/\"\n",
    "\n",
    "            headers = {\"cookie\": f\"_pinterest_sess={pinterest_sess};\"}\n",
    "\n",
    "            async def make_request(query, pinterest_sess, bookmark=None):\n",
    "                params = {\n",
    "                    'data': '{\"options\":{\"article\":null,\"applied_filters\":{\"filters\":[{\"filter_id\":\"4453471066647658309\",\"filter_options\":[{\"filter_option_id\":\"-4738628780439854060\",\"string_value\":\"/search/boards/?q=Messi and ronaldo&rs=content_type_filter\"}]}]},\"appliedProductFilters\":\"---\",\"auto_correction_disabled\":false,\"corpus\":null,\"customized_rerank_type\":null,\"filters\":null,\"query\":\"Messi and ronaldo\",\"query_pin_sigs\":null,\"redux_normalize_feed\":true,\"rs\":\"typed\",\"scope\":\"boards\",\"source_id\":null},\"context\":{}}',\n",
    "                }\n",
    "                querystring_dict = json.loads(params[\"data\"])\n",
    "                querystring_dict[\"options\"][\"query\"] = query\n",
    "                querystring_dict['options']['applied_filters']['filters'][0]['filter_options'][0]['string_value'] = f'/search/boards/?q={query}&rs=content_type_filter'\n",
    "\n",
    "                if bookmark != None:\n",
    "                    querystring_dict[\"options\"][\"bookmarks\"] = [bookmark]\n",
    "                    # print(f\"[*] Loading new boards {bookmark}\")\n",
    "                querystring = {\"data\": json.dumps(querystring_dict)}\n",
    "\n",
    "                # # Stop scraping if scraped moe than 1000 boards per user\n",
    "                # if len(boards) >= 20000:\n",
    "                #     return\n",
    "\n",
    "                sslcontext = ssl.create_default_context(cafile=certifi.where())\n",
    "                # timeout = aiohttp.ClientTimeout(\n",
    "                #     total=10, connect=None, sock_connect=None, sock_read=None\n",
    "                # )\n",
    "                async with aiohttp.ClientSession(\n",
    "                    trust_env=True\n",
    "                ) as session:\n",
    "                    async with session.get(\n",
    "                        main_url,\n",
    "                        params=querystring,\n",
    "                        proxy=\"http://Hasamba:jrdy6mtgMbVDEobe4R@pr.oxylabs.io:7777\",\n",
    "                        ssl=sslcontext,\n",
    "                        headers=headers,\n",
    "                    ) as response:\n",
    "                        if not response.ok:\n",
    "                            print(response)\n",
    "                            return\n",
    "\n",
    "                        response = await response.json()\n",
    "                        for item in response[\"resource_response\"][\"data\"][\"results\"]:\n",
    "                            try:\n",
    "                                found_kws = []\n",
    "                                for kw_value in kw_values:\n",
    "                                  if ' '+kw_value[0].lower()+' ' in ' '+str(item[\"name\"]).lower()+' ':\n",
    "                                    found_kws.append(kw_value[0])\n",
    "                            except Exception as e:\n",
    "                              print(e)\n",
    "                              print(repr(e))\n",
    "                              print(traceback.format_exc())\n",
    "                              continue\n",
    "                            try:\n",
    "                                boards.append(\n",
    "                                    SimpleSearchBoard(\n",
    "                                        board_id=item[\"id\"],\n",
    "                                        board_url=f\"pinterest.com{item['url']}\",\n",
    "                                        board_title=item[\"name\"],\n",
    "                                        author_id=item[\"owner\"][\"id\"],\n",
    "                                        author_name=item[\"owner\"][\"full_name\"],\n",
    "                                        author_url=f\"pinterest.com/{item['owner']['username']}/\",\n",
    "                                        author_number_of_followers=item[\"owner\"][\n",
    "                                            \"follower_count\"\n",
    "                                        ],\n",
    "                                        pin_count=item[\"pin_count\"],\n",
    "                                        search_query=response[\"resource\"][\"options\"][\n",
    "                                            \"query\"\n",
    "                                        ],\n",
    "                                        # search_query_translation = translate_text(response[\"resource\"][\"options\"][\n",
    "                                        #     \"query\"\n",
    "                                        # ]),\n",
    "                                        # account_language = detect_language(item[\"name\"]),\n",
    "                                        # board_title_translation = translate_text(item[\"name\"]),\n",
    "                                        risk_score = len(found_kws),\n",
    "                                    )\n",
    "                                )\n",
    "                                print(\n",
    "                                    f\"[*] Scraped -> pinterest.com{item['url']}\"\n",
    "                                )\n",
    "                            except Exception as e:\n",
    "                                print(repr(e))\n",
    "                                continue\n",
    "                        if not response.get(\"resource_response\").get(\"bookmark\"):\n",
    "                            return\n",
    "\n",
    "                        return await make_request(\n",
    "                            query, pinterest_sess, response[\"resource_response\"][\"bookmark\"], \n",
    "                        )\n",
    "\n",
    "            await make_request(query, pinterest_sess=pinterest_sess)\n",
    "            print(f\"[*] Finished scraping {query} boards (found {len(boards)} boards)\")\n",
    "            return boards\n",
    "        except Exception as e:\n",
    "            print(repr(e))\n",
    "            print(traceback.format_exc())\n",
    "            continue\n",
    "    return []\n",
    "\n",
    "async def scrape_search_results_boards(queries, pinterest_sess, batch_size=50, specific_save_path=None):\n",
    "    random_id = uuid.uuid1()\n",
    "    print(f\"s3://activefence-user/jakubs/pinterest/search-results-boards-{random_id}\")\n",
    "    time.sleep(3)\n",
    "\n",
    "    columns = ['board_id', 'author_name', 'author_number_of_followers', 'author_url', 'author_id', 'board_title', 'board_url', 'pin_count', 'search_query', 'search_query_translation', 'account_language',   'board_title_translation','risk_score']\n",
    "\n",
    "    simple_columns = ['board_id', 'author_name', 'author_number_of_followers', 'author_url', 'author_id', 'board_title', 'board_url', 'pin_count', 'search_query','risk_score']\n",
    "\n",
    "    iterator = 0\n",
    "    for i in range(0, len(queries), batch_size):\n",
    "        try:\n",
    "            iterator += 1\n",
    "            batch = queries[i : i + batch_size]\n",
    "            # print(\"Last item\", queries[i : i + batch_size][-1])\n",
    "            tasks = [get_search_result_boards(query, pinterest_sess) for query in batch]\n",
    "            batch_results = await asyncio.gather(*tasks, return_exceptions=True)\n",
    "            batch_results = [result for result in batch_results if not isinstance(result, Exception)]\n",
    "\n",
    "            batch_results = list(chain(*batch_results))\n",
    "\n",
    "            print(\n",
    "                f\"[*] Scraped {len(batch_results)} boards in batch #{iterator} / out of #{len(range(0, len(queries), batch_size))}\"\n",
    "            )\n",
    "\n",
    "            if len(batch_results) == 0:\n",
    "                print(\"Batch results are 0 - continue.\")\n",
    "                continue\n",
    "            \n",
    "            # print(\"rdd.\")\n",
    "            rdd = spark.sparkContext.parallelize(batch_results)\n",
    "            batch_results_df = rdd.toDF(simple_columns)\n",
    "\n",
    "            if specific_save_path is None:\n",
    "                batch_results_df.repartition(1000).write.mode(\"append\").format(\n",
    "                    \"parquet\"\n",
    "                ).save(\n",
    "                    f\"s3://activefence-user/jakubs/pinterest/search-results-boards-{random_id}\"\n",
    "                )\n",
    "            else:\n",
    "                batch_results_df.repartition(1000).write.mode(\"append\").format(\n",
    "                    \"parquet\"\n",
    "                ).save(specific_save_path)\n",
    "\n",
    "        except Exception as e:\n",
    "            print(f\"An error occurred during the processing of batch #{iterator}\")\n",
    "            print(traceback.format_exc())\n",
    "            continue\n",
    "\n",
    "    return f\"s3://activefence-user/jakubs/pinterest/search-results-boards-{random_id}\"\n",
    "  \n",
    "\n",
    "async def get_followees(account_name, _pinterest_sess):\n",
    "    followees = []\n",
    "\n",
    "    main_url = \"https://br.pinterest.com/resource/UserFollowingResource/get/\"\n",
    "    headers = {\"cookie\": f\"_pinterest_sess={_pinterest_sess};\"}\n",
    "\n",
    "    async def make_request(account_name, _pinterest_sess, bookmark=None):\n",
    "        querystring = {\n",
    "            \"data\": '{\"options\":{\"page_size\":50,\"explicit_following\":false,\"username\":\"myswisspanorama\"}}',\n",
    "        }\n",
    "        querystring_dict = json.loads(querystring[\"data\"])\n",
    "        querystring_dict[\"options\"][\"username\"] = account_name\n",
    "\n",
    "\n",
    "        if bookmark != None:\n",
    "            querystring_dict[\"options\"][\"bookmarks\"] = [bookmark]\n",
    "        querystring = {\"data\": json.dumps(querystring_dict)}\n",
    "\n",
    "\n",
    "        async with aiohttp.ClientSession() as session:\n",
    "            async with session.get(\n",
    "                main_url, params=querystring, headers=headers, proxy=\"http://Hasamba:jrdy6mtgMbVDEobe4R@pr.oxylabs.io:7777\",\n",
    "            ) as response:\n",
    "                if not response.ok:\n",
    "                    return\n",
    "                response = await response.json()\n",
    "                # print(response)\n",
    "                for item in response[\"resource_response\"][\"data\"]:\n",
    "                    try:\n",
    "                        followees_data = await get_account_data(item[\"username\"])\n",
    "                        followees.append(\n",
    "                        Follower(\n",
    "                                source_username=account_name,\n",
    "                                follower_id=item[\"id\"],\n",
    "                                follower_username=item[\"username\"],\n",
    "                                follower_name=item[\"full_name\"],\n",
    "                                follower_url=f\"https://pinterest.com/{item['username']}/\",\n",
    "                                number_of_follower_followers= followees_data[\"resource_response\"][\"data\"][\"follower_count\"],\n",
    "                                pin_count = followees_data[\"resource_response\"]['data']['pin_count'],\n",
    "                                connection_type = \"Followee\",\n",
    "                            )\n",
    "                        )\n",
    "                        print(f\"[*] Scraped -> https://pinterest.com/{item['username']}/\" )\n",
    "                    except Exception as e:\n",
    "                        print(e)\n",
    "                if not response.get(\"resource_response\").get(\"bookmark\"):\n",
    "                    return\n",
    "\n",
    "                await make_request(\n",
    "                    account_name,\n",
    "                    _pinterest_sess,\n",
    "                    response[\"resource_response\"][\"bookmark\"],\n",
    "                )\n",
    "\n",
    "    await make_request(account_name, _pinterest_sess)\n",
    "    print(\n",
    "        f\"[*] Finished scraping followees  {account_name} (found {len(followees)} followees)\"\n",
    "    )\n",
    "    return followees"
   ]
  }
 ],
 "metadata": {
  "application/vnd.databricks.v1+notebook": {
   "dashboards": [],
   "language": "python",
   "notebookMetadata": {
    "pythonIndentUnit": 2
   },
   "notebookName": "Pinterest Scraper",
   "widgets": {}
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
